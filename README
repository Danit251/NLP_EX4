The plan for RE:
Examples:
    (1) work for:
      Yoav BIU

    (2) Live in:
       Danit Givataim

Pipeline:
1. Identify Ner - for (1) we need PERSON and ORG (and MISC):

                  Reagen -> We (Corref) in the American system (34)
                  the R1 and R2 can be in different clauses. Nixon U.S. (50)
                  Consectuve GPE PER -> we will match together -  STOCKTON-ON-TEES , England (AP) _ Michael Minns [...] (70)
                  this relation can be also with NORP that indicates adjective. (79)
                  for (2) We need PER and LOC (maybe also MISC)
                  Person and GPE that have prepositional relation (prep) (78)
                  compoumd relation between ORGs = the person work_for both (78)

                  Counter example - 35, 52, 60

2.
train
flair score 85.99033816425121%
spacy score 75.84541062801932%
combined score 93.71980676328504%
dev
flair score 80.59701492537313%
spacy score 70.1492537313433%
combined score 86.06965174129353%

3. RULE Based:  NP.Chunks:
    A.
    ->after gathering all person and organization entities:
    -> We iterate through the np chunks. and iterate on the product of PER and ORG
    -> in in a chunk there is both org and per. We declare this person and org are in work_for relation
    Success Rate on Train:
    from  109 work_for relation 17 are matching the relation:
    17/17 are real work_for relation (100%) when we manged to have 13/17 exact match (due the ner extractor not aligned )
    From all other sentences that not contains the work_for relation there are 4 such chunks that match the rule
    but not in work_for relation but after error analysis it is actually 3 un tagged work_for relation and 1 that is a
    real mistake.
    in total: 13/21  in naive exact match
              17/21 in a fuzzy evaluation
              20/21 in smart human evaluation
              Because we don't know how the test would looks like we will take it as 20/21. --> very good rule
